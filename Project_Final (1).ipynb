{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STjbOFgzjKN7",
        "outputId": "7660d737-923d-4440-8213-9f681f04b9ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'face_attribute_attack'...\n",
            "remote: Enumerating objects: 193, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 193 (delta 0), reused 1 (delta 0), pack-reused 190\u001b[K\n",
            "Receiving objects: 100% (193/193), 34.30 MiB | 28.35 MiB/s, done.\n",
            "Resolving deltas: 100% (43/43), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/satwiksunnam19/face_attribute_attack.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obglByA2jSem",
        "outputId": "25def8f8-6c88-4b48-aa5d-c41ab2e48e5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/face_attribute_attack\n",
            "Collecting dlib==19.24.0 (from -r requirements.txt (line 1))\n",
            "  Downloading dlib-19.24.0.tar.gz (3.2 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting lpips==0.1.4 (from -r requirements.txt (line 2))\n",
            "  Downloading lpips-0.1.4-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting matplotlib==3.5.2 (from -r requirements.txt (line 3))\n",
            "  Downloading matplotlib-3.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.9 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opencv-python==4.6.0.66 (from -r requirements.txt (line 4))\n",
            "  Downloading opencv_python-4.6.0.66-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.9 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.9/60.9 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas==1.3.5 (from -r requirements.txt (line 5))\n",
            "  Downloading pandas-1.3.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Pillow==9.0.1 (from -r requirements.txt (line 6))\n",
            "  Downloading Pillow-9.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protobuf==3.19.4 (from -r requirements.txt (line 7))\n",
            "  Downloading protobuf-3.19.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyYAML==6.0 (from -r requirements.txt (line 8))\n",
            "  Downloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (682 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m682.2/682.2 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting regex==2023.6.3 (from -r requirements.txt (line 9))\n",
            "  Downloading regex-2023.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m770.4/770.4 kB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests==2.28.1 (from -r requirements.txt (line 10))\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-image==0.19.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (0.19.3)\n",
            "Collecting scikit-learn==1.0.2 (from -r requirements.txt (line 12))\n",
            "  Downloading scikit_learn-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy==1.7.3 (from -r requirements.txt (line 13))\n",
            "  Downloading scipy-1.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.9 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Ignored the following versions that require a different python version: 1.6.2 Requires-Python >=3.7,<3.10; 1.6.3 Requires-Python >=3.7,<3.10; 1.7.0 Requires-Python >=3.7,<3.10; 1.7.1 Requires-Python >=3.7,<3.10\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.8.1 (from versions: 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.8.1\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%cd face_attribute_attack\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "3ba7THFTjWan",
        "outputId": "1e555b42-7d0e-47c4-9682-10d7ee9149f2"
      },
      "outputs": [
        {
          "ename": "IsADirectoryError",
          "evalue": "[Errno 21] Is a directory: '/content/drive/MyDrive/images1024x1024/00000'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-a0d5ebb7818c>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mdestination\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_folder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestination\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfolder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfolder2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m     \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m     \u001b[0mcopymode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/content/drive/MyDrive/images1024x1024/00000'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Path to the main folder\n",
        "main_folder = '/content/drive/MyDrive/images1024x1024'\n",
        "\n",
        "# List all folders in the main folder\n",
        "folders = [f for f in os.listdir(main_folder) if os.path.isdir(os.path.join(main_folder, f))]\n",
        "\n",
        "half_len = 20\n",
        "folder1 = folders[:half_len]\n",
        "folder2 = folders[half_len:40]\n",
        "\n",
        "# Create new folders\n",
        "new_folder1 = os.path.join(main_folder, '/content/face_attribute_attack/data/train/real')\n",
        "new_folder2 = os.path.join(main_folder, '/content/face_attribute_attack/data/test/real')\n",
        "os.makedirs(new_folder1, exist_ok=True)\n",
        "os.makedirs(new_folder2, exist_ok=True)\n",
        "\n",
        "# Copy contents of folders to new folders\n",
        "for folder in folder1:\n",
        "    source = os.path.join(main_folder, folder)\n",
        "    destination = os.path.join(new_folder1, folder)\n",
        "    shutil.copy(source, destination)\n",
        "\n",
        "for folder in folder2:\n",
        "    source = os.path.join(main_folder, folder)\n",
        "    destination = os.path.join(new_folder2, folder)\n",
        "    shutil.copy(source, destination)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NpXOVs_-kRIo"
      },
      "outputs": [],
      "source": [
        "c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "aVHN_3h8k4Ze"
      },
      "outputs": [],
      "source": [
        "! cp /content/drive/MyDrive/stylegan2-ffhq-config-f.pt /content/face_attribute_attack/pretrained_models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qs-WWiPjs_Qk",
        "outputId": "2e4fdef5-74a7-4c5d-c619-dcd334882f8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.16.6-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.44.1-py2.py3-none-any.whl (266 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m266.1/266.1 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.43 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.44.1 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.6\n"
          ]
        }
      ],
      "source": [
        "! pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vehwl9JxpHO6",
        "outputId": "d5b5d171-a919-4651-876e-e009688980fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/face_attribute_attack/wandb/run-20240404_170706-efficientnet_forensic_classifier\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Resuming run \u001b[33mefficientnet_forensic_classifier\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/collide-conquer19/forensic_classifier\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/collide-conquer19/forensic_classifier/runs/efficientnet_forensic_classifier\u001b[0m\n",
            "Checkpoint file not found at path: forensic_classifier_trained_models/efficientnet/best_epoch.pt. Training from scratch.\n",
            "  0% 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "epoch : 0, step: 0, loss : 0.6887516975402832\n",
            "epoch : 0, step: 1, loss : 0.9766124486923218\n",
            "epoch : 0, step: 2, loss : 0.7549165487289429\n",
            "epoch : 0, step: 3, loss : 0.7456969022750854\n",
            "epoch : 0, step: 4, loss : 0.8033938407897949\n",
            "epoch : 0, step: 5, loss : 0.7133438587188721\n",
            "epoch : 0, step: 6, loss : 0.7420741319656372\n",
            "epoch : 0, step: 7, loss : 0.7471233606338501\n",
            "epoch : 0, step: 8, loss : 0.7230730652809143\n",
            "epoch : 0, step: 9, loss : 0.6996607184410095\n",
            "epoch : 0, step: 10, loss : 0.698215126991272\n",
            "epoch : 0, step: 11, loss : 0.7057849168777466\n",
            "epoch : 0, step: 12, loss : 0.7045577764511108\n",
            "epoch : 0, step: 13, loss : 0.6897913813591003\n",
            "epoch : 0, step: 14, loss : 0.6742777228355408\n",
            "epoch : 0, step: 15, loss : 0.6835477948188782\n",
            "epoch : 0, step: 16, loss : 0.6855024099349976\n",
            "epoch : 0, step: 17, loss : 0.6876587867736816\n",
            "epoch : 0, step: 18, loss : 0.7079082131385803\n",
            "epoch : 0, step: 19, loss : 0.694004237651825\n",
            "epoch : 0, step: 20, loss : 0.7206783294677734\n",
            "epoch : 0, step: 21, loss : 0.7131718397140503\n",
            "epoch : 0, step: 22, loss : 0.6996523141860962\n",
            "epoch : 0, step: 23, loss : 0.6982793211936951\n",
            "epoch : 0, step: 24, loss : 0.7045203447341919\n",
            "epoch : 0, step: 25, loss : 0.7047262191772461\n",
            "epoch : 0, step: 26, loss : 0.7076261639595032\n",
            "epoch : 0, step: 27, loss : 0.7066096663475037\n",
            "epoch : 0, step: 28, loss : 0.6884663701057434\n",
            "epoch : 0, step: 29, loss : 0.6866971254348755\n",
            "epoch : 0, step: 30, loss : 0.7585198283195496\n",
            "epoch : 0, step: 31, loss : 0.7219318151473999\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Accuracy of the network on the 2000 test images: 48.0 %\n",
            "Saving best epoch at epoch 0\n",
            "100% 1/1 [05:43<00:00, 343.01s/it]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  step â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         test_accuracy â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_classifier_loss â–â–ˆâ–ƒâ–ƒâ–„â–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–ƒâ–‚\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  step 31\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         test_accuracy 48.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_classifier_loss 0.72193\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run \u001b[33mefficientnet_forensic_classifier\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/collide-conquer19/forensic_classifier/runs/efficientnet_forensic_classifier\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/collide-conquer19/forensic_classifier\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240404_170706-efficientnet_forensic_classifier/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python classifier_training.py --train_data data/train --test_data data/test  --batch_size 128  --epochs 5  --classifier_name resnet50  --output_path forensic_classifier_trained_models/resnet50/  --wandb_project_name forensic_classifier --experiment_name efficientnet_forensic_classifier --resume_training False\n",
        "# Note: The trained model will be saved in the output_path under the name 'best_epoch.pt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BT7LmMumFIxA"
      },
      "outputs": [],
      "source": [
        "! cp /content/face_attribute_attack -r /content/drive/MyDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "bNtCiz6VHX_T"
      },
      "outputs": [],
      "source": [
        "! cp /content/face_attribute_attack/forensic_classifier_trained_models/efficientnet/best_epoch.pt -r /content/face_attribute_attack/forensic_classifier_trained_models/resnet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ubgWCZd2KGbD"
      },
      "outputs": [],
      "source": [
        "! cp /content/face_attribute_attack/data/train/real/00000/00000.png -r /content/face_attribute_attack/reference_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3o4IyNlKTMtC",
        "outputId": "bd2e0231-ab51-4582-baab-e8ea1c27f84a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ftfy\n",
            "  Downloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n",
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/54.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.13)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.2.0\n"
          ]
        }
      ],
      "source": [
        "! pip install ftfy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKLT2wyds7Tl",
        "outputId": "2c6c8c08-f7cc-4a77-f81b-549d779c6753"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Iteration: 0, Total Loss: 0.802734375, CLIP Loss: 0.7998046875, L2 Loss: 0.0, Forensic Classifier Loss: 0.6257566809654236, wb model prediction: 0.5348565578460693\n",
            "Iteration: 1, Total Loss: 0.79931640625, CLIP Loss: 0.79638671875, L2 Loss: 0.009212153032422066, Forensic Classifier Loss: 0.5894679427146912, wb model prediction: 0.5546222925186157\n",
            "Iteration: 2, Total Loss: 0.79638671875, CLIP Loss: 0.79345703125, L2 Loss: 0.02910512685775757, Forensic Classifier Loss: 0.5612735152244568, wb model prediction: 0.570482075214386\n",
            "Iteration: 3, Total Loss: 0.79296875, CLIP Loss: 0.7900390625, L2 Loss: 0.0595337450504303, Forensic Classifier Loss: 0.5901151299476624, wb model prediction: 0.5542634725570679\n",
            "Iteration: 4, Total Loss: 0.7900390625, CLIP Loss: 0.787109375, L2 Loss: 0.09695082902908325, Forensic Classifier Loss: 0.5799249410629272, wb model prediction: 0.5599403977394104\n",
            "Iteration: 5, Total Loss: 0.787109375, CLIP Loss: 0.7841796875, L2 Loss: 0.14356133341789246, Forensic Classifier Loss: 0.5742934942245483, wb model prediction: 0.5631025433540344\n",
            "Iteration: 6, Total Loss: 0.78369140625, CLIP Loss: 0.78076171875, L2 Loss: 0.19810625910758972, Forensic Classifier Loss: 0.6149724125862122, wb model prediction: 0.540655791759491\n",
            "Iteration: 7, Total Loss: 0.77978515625, CLIP Loss: 0.77685546875, L2 Loss: 0.2627291977405548, Forensic Classifier Loss: 0.6099379062652588, wb model prediction: 0.5433846116065979\n",
            "Iteration: 8, Total Loss: 0.7763671875, CLIP Loss: 0.7734375, L2 Loss: 0.33690184354782104, Forensic Classifier Loss: 0.5970742702484131, wb model prediction: 0.5504196286201477\n",
            "Iteration: 9, Total Loss: 0.77294921875, CLIP Loss: 0.77001953125, L2 Loss: 0.4206860065460205, Forensic Classifier Loss: 0.6154623031616211, wb model prediction: 0.5403910279273987\n",
            "Iteration: 10, Total Loss: 0.77001953125, CLIP Loss: 0.7666015625, L2 Loss: 0.5132258534431458, Forensic Classifier Loss: 0.601872980594635, wb model prediction: 0.547784686088562\n",
            "Iteration: 11, Total Loss: 0.76708984375, CLIP Loss: 0.763671875, L2 Loss: 0.6141834259033203, Forensic Classifier Loss: 0.5764511227607727, wb model prediction: 0.5618888735771179\n",
            "Iteration: 12, Total Loss: 0.763671875, CLIP Loss: 0.76025390625, L2 Loss: 0.7235385775566101, Forensic Classifier Loss: 0.5826574563980103, wb model prediction: 0.5584124326705933\n",
            "Iteration: 13, Total Loss: 0.76025390625, CLIP Loss: 0.7568359375, L2 Loss: 0.8406054973602295, Forensic Classifier Loss: 0.5815590620040894, wb model prediction: 0.5590261220932007\n",
            "Iteration: 14, Total Loss: 0.75732421875, CLIP Loss: 0.75390625, L2 Loss: 0.9664878845214844, Forensic Classifier Loss: 0.5793564915657043, wb model prediction: 0.5602588057518005\n",
            "Iteration: 15, Total Loss: 0.75390625, CLIP Loss: 0.75048828125, L2 Loss: 1.100999355316162, Forensic Classifier Loss: 0.5868017673492432, wb model prediction: 0.556102991104126\n",
            "Iteration: 16, Total Loss: 0.75048828125, CLIP Loss: 0.7470703125, L2 Loss: 1.2431259155273438, Forensic Classifier Loss: 0.578942596912384, wb model prediction: 0.5604907274246216\n",
            "Iteration: 17, Total Loss: 0.7470703125, CLIP Loss: 0.74365234375, L2 Loss: 1.3913545608520508, Forensic Classifier Loss: 0.5618239641189575, wb model prediction: 0.570168137550354\n",
            "Iteration: 18, Total Loss: 0.744140625, CLIP Loss: 0.740234375, L2 Loss: 1.546079158782959, Forensic Classifier Loss: 0.5705902576446533, wb model prediction: 0.5651917457580566\n",
            "Iteration: 19, Total Loss: 0.7412109375, CLIP Loss: 0.7373046875, L2 Loss: 1.708977222442627, Forensic Classifier Loss: 0.5837425589561462, wb model prediction: 0.5578068494796753\n",
            "Iteration: 20, Total Loss: 0.73828125, CLIP Loss: 0.734375, L2 Loss: 1.8788100481033325, Forensic Classifier Loss: 0.6085543036460876, wb model prediction: 0.5441369414329529\n",
            "Iteration: 21, Total Loss: 0.73486328125, CLIP Loss: 0.73095703125, L2 Loss: 2.0534286499023438, Forensic Classifier Loss: 0.5924030542373657, wb model prediction: 0.552996814250946\n",
            "Iteration: 22, Total Loss: 0.73193359375, CLIP Loss: 0.72802734375, L2 Loss: 2.2321715354919434, Forensic Classifier Loss: 0.5898419618606567, wb model prediction: 0.5544149279594421\n",
            "Iteration: 23, Total Loss: 0.7294921875, CLIP Loss: 0.7255859375, L2 Loss: 2.4125418663024902, Forensic Classifier Loss: 0.5922624468803406, wb model prediction: 0.5530745387077332\n",
            "Iteration: 24, Total Loss: 0.72705078125, CLIP Loss: 0.72265625, L2 Loss: 2.5956664085388184, Forensic Classifier Loss: 0.568011999130249, wb model prediction: 0.5666508078575134\n",
            "Iteration: 25, Total Loss: 0.724609375, CLIP Loss: 0.72021484375, L2 Loss: 2.779250144958496, Forensic Classifier Loss: 0.5829336643218994, wb model prediction: 0.5582582354545593\n",
            "Iteration: 26, Total Loss: 0.72216796875, CLIP Loss: 0.7177734375, L2 Loss: 2.9624974727630615, Forensic Classifier Loss: 0.6008956432342529, wb model prediction: 0.5483203530311584\n",
            "Iteration: 27, Total Loss: 0.71923828125, CLIP Loss: 0.71484375, L2 Loss: 3.146329402923584, Forensic Classifier Loss: 0.5911563038825989, wb model prediction: 0.5536866784095764\n",
            "Iteration: 28, Total Loss: 0.716796875, CLIP Loss: 0.71240234375, L2 Loss: 3.3294947147369385, Forensic Classifier Loss: 0.6045922636985779, wb model prediction: 0.5462971329689026\n",
            "Iteration: 29, Total Loss: 0.71484375, CLIP Loss: 0.7099609375, L2 Loss: 3.5133655071258545, Forensic Classifier Loss: 0.5769355893135071, wb model prediction: 0.5616167783737183\n",
            "Iteration: 30, Total Loss: 0.7119140625, CLIP Loss: 0.70703125, L2 Loss: 3.6947038173675537, Forensic Classifier Loss: 0.5695593357086182, wb model prediction: 0.56577467918396\n",
            "Iteration: 31, Total Loss: 0.7099609375, CLIP Loss: 0.705078125, L2 Loss: 3.875746250152588, Forensic Classifier Loss: 0.5736221075057983, wb model prediction: 0.5634807348251343\n",
            "Iteration: 32, Total Loss: 0.70751953125, CLIP Loss: 0.70263671875, L2 Loss: 4.059006690979004, Forensic Classifier Loss: 0.5706984996795654, wb model prediction: 0.5651305317878723\n",
            "Iteration: 33, Total Loss: 0.70556640625, CLIP Loss: 0.70068359375, L2 Loss: 4.243324279785156, Forensic Classifier Loss: 0.5668458938598633, wb model prediction: 0.5673120021820068\n",
            "Iteration: 34, Total Loss: 0.70361328125, CLIP Loss: 0.6982421875, L2 Loss: 4.427093029022217, Forensic Classifier Loss: 0.5825808644294739, wb model prediction: 0.5584551692008972\n",
            "Iteration: 35, Total Loss: 0.70166015625, CLIP Loss: 0.6962890625, L2 Loss: 4.610062599182129, Forensic Classifier Loss: 0.5779284834861755, wb model prediction: 0.5610594153404236\n",
            "Iteration: 36, Total Loss: 0.69970703125, CLIP Loss: 0.6943359375, L2 Loss: 4.790016174316406, Forensic Classifier Loss: 0.5812869071960449, wb model prediction: 0.5591782927513123\n",
            "Iteration: 37, Total Loss: 0.69775390625, CLIP Loss: 0.6923828125, L2 Loss: 4.968575954437256, Forensic Classifier Loss: 0.5629311203956604, wb model prediction: 0.5695372223854065\n",
            "Iteration: 38, Total Loss: 0.69580078125, CLIP Loss: 0.6904296875, L2 Loss: 5.147873401641846, Forensic Classifier Loss: 0.5586742162704468, wb model prediction: 0.5719668865203857\n",
            "Iteration: 39, Total Loss: 0.693359375, CLIP Loss: 0.68798828125, L2 Loss: 5.330080509185791, Forensic Classifier Loss: 0.5795261263847351, wb model prediction: 0.5601637363433838\n",
            "Iteration: 40, Total Loss: 0.69189453125, CLIP Loss: 0.68603515625, L2 Loss: 5.51406192779541, Forensic Classifier Loss: 0.5671548843383789, wb model prediction: 0.5671367049217224\n",
            "Iteration: 41, Total Loss: 0.6904296875, CLIP Loss: 0.6845703125, L2 Loss: 5.69782829284668, Forensic Classifier Loss: 0.5751489400863647, wb model prediction: 0.5626210570335388\n",
            "Iteration: 42, Total Loss: 0.68798828125, CLIP Loss: 0.68212890625, L2 Loss: 5.882850646972656, Forensic Classifier Loss: 0.5618072748184204, wb model prediction: 0.5701776742935181\n",
            "Iteration: 43, Total Loss: 0.6865234375, CLIP Loss: 0.6806640625, L2 Loss: 6.068833827972412, Forensic Classifier Loss: 0.5425249934196472, wb model prediction: 0.5812786817550659\n",
            "Iteration: 44, Total Loss: 0.6845703125, CLIP Loss: 0.6787109375, L2 Loss: 6.253608226776123, Forensic Classifier Loss: 0.5476210117340088, wb model prediction: 0.578324019908905\n",
            "Iteration: 45, Total Loss: 0.68310546875, CLIP Loss: 0.67724609375, L2 Loss: 6.436225891113281, Forensic Classifier Loss: 0.52410489320755, wb model prediction: 0.5920851230621338\n",
            "Iteration: 46, Total Loss: 0.68212890625, CLIP Loss: 0.67578125, L2 Loss: 6.612839698791504, Forensic Classifier Loss: 0.5399971604347229, wb model prediction: 0.5827499032020569\n",
            "Iteration: 47, Total Loss: 0.67919921875, CLIP Loss: 0.67333984375, L2 Loss: 6.785481929779053, Forensic Classifier Loss: 0.5108923316001892, wb model prediction: 0.5999599695205688\n",
            "Iteration: 48, Total Loss: 0.677734375, CLIP Loss: 0.671875, L2 Loss: 6.954798698425293, Forensic Classifier Loss: 0.5159163475036621, wb model prediction: 0.5969533324241638\n",
            "Iteration: 49, Total Loss: 0.67529296875, CLIP Loss: 0.6689453125, L2 Loss: 7.121457099914551, Forensic Classifier Loss: 0.562665581703186, wb model prediction: 0.5696884393692017\n"
          ]
        }
      ],
      "source": [
        "! python text_as_reference.py --config_file configs/config_text_as_reference.ini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qF1BVyTgUvYl",
        "outputId": "aec31571-76d9-4a92-83af-45f65d68f9a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting lpips\n",
            "  Using cached lpips-0.1.4-py3-none-any.whl (53 kB)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from lpips) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from lpips) (0.17.1+cu121)\n",
            "Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.10/dist-packages (from lpips) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from lpips) (1.11.4)\n",
            "Requirement already satisfied: tqdm>=4.28.1 in /usr/local/lib/python3.10/dist-packages (from lpips) (4.66.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=0.4.0->lpips)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=0.4.0->lpips)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=0.4.0->lpips)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=0.4.0->lpips)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=0.4.0->lpips)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=0.4.0->lpips)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=0.4.0->lpips)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=0.4.0->lpips)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=0.4.0->lpips)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=0.4.0->lpips)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=0.4.0->lpips)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=0.4.0->lpips)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.2.1->lpips) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.0->lpips) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.0->lpips) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, lpips\n",
            "Successfully installed lpips-0.1.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "! pip install lpips"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43_huA0rBH6J",
        "outputId": "6ada66b6-032e-414e-ee46-2b0601876937"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/vgg.pth\n",
            "[1/50] Loss: 0.750751256942749 Perceptual Loss: 0.7478598356246948 L2 Loss: 0.0 Forensic Classifier Loss: 0.5782790184020996, WB Prediction: 0.5608627796173096\n",
            "[2/50] Loss: 0.7468724250793457 Perceptual Loss: 0.7437586188316345 L2 Loss: 0.2047729790210724 Forensic Classifier Loss: 0.6022821068763733, WB Prediction: 0.5475606322288513\n",
            "[3/50] Loss: 0.7448517084121704 Perceptual Loss: 0.7416473627090454 L2 Loss: 0.5754818916320801 Forensic Classifier Loss: 0.5833296775817871, WB Prediction: 0.5580371618270874\n",
            "[4/50] Loss: 0.7425946593284607 Perceptual Loss: 0.7391321659088135 L2 Loss: 0.9683679342269897 Forensic Classifier Loss: 0.5956650972366333, WB Prediction: 0.5511958599090576\n",
            "[5/50] Loss: 0.739154040813446 Perceptual Loss: 0.7353042960166931 L2 Loss: 1.3854568004608154 Forensic Classifier Loss: 0.6313982605934143, WB Prediction: 0.5318475961685181\n",
            "[6/50] Loss: 0.7368598580360413 Perceptual Loss: 0.7326800227165222 L2 Loss: 1.841421365737915 Forensic Classifier Loss: 0.651818573474884, WB Prediction: 0.5210972428321838\n",
            "[7/50] Loss: 0.7341535687446594 Perceptual Loss: 0.7298392057418823 L2 Loss: 2.378574848175049 Forensic Classifier Loss: 0.6250197291374207, WB Prediction: 0.5352509021759033\n",
            "[8/50] Loss: 0.7318502068519592 Perceptual Loss: 0.7273073792457581 L2 Loss: 3.0182559490203857 Forensic Classifier Loss: 0.6067354679107666, WB Prediction: 0.5451275706291199\n",
            "[9/50] Loss: 0.7316107749938965 Perceptual Loss: 0.726548433303833 L2 Loss: 3.6598610877990723 Forensic Classifier Loss: 0.6464847326278687, WB Prediction: 0.5238841772079468\n",
            "[10/50] Loss: 0.7292740941047668 Perceptual Loss: 0.7238541841506958 L2 Loss: 4.406139850616455 Forensic Classifier Loss: 0.6433696150779724, WB Prediction: 0.5255186557769775\n",
            "[11/50] Loss: 0.7284846305847168 Perceptual Loss: 0.7227318286895752 L2 Loss: 5.14273738861084 Forensic Classifier Loss: 0.6362894773483276, WB Prediction: 0.5292525887489319\n",
            "[12/50] Loss: 0.7272517681121826 Perceptual Loss: 0.7212982773780823 L2 Loss: 5.75407075881958 Forensic Classifier Loss: 0.6152884364128113, WB Prediction: 0.5404849648475647\n",
            "[13/50] Loss: 0.7260338068008423 Perceptual Loss: 0.7197057604789734 L2 Loss: 6.411044120788574 Forensic Classifier Loss: 0.6245063543319702, WB Prediction: 0.5355257391929626\n",
            "[14/50] Loss: 0.7254577279090881 Perceptual Loss: 0.7186357975006104 L2 Loss: 7.0985822677612305 Forensic Classifier Loss: 0.654533863067627, WB Prediction: 0.5196842551231384\n",
            "[15/50] Loss: 0.7244906425476074 Perceptual Loss: 0.717622697353363 L2 Loss: 7.797662734985352 Forensic Classifier Loss: 0.5938228964805603, WB Prediction: 0.5522121787071228\n",
            "[16/50] Loss: 0.7247467637062073 Perceptual Loss: 0.7174723148345947 L2 Loss: 8.53436279296875 Forensic Classifier Loss: 0.6014540791511536, WB Prediction: 0.548014223575592\n",
            "[17/50] Loss: 0.7238131165504456 Perceptual Loss: 0.7165026068687439 L2 Loss: 9.332047462463379 Forensic Classifier Loss: 0.5288958549499512, WB Prediction: 0.5892552137374878\n",
            "[18/50] Loss: 0.7228500843048096 Perceptual Loss: 0.7151762843132019 L2 Loss: 10.069846153259277 Forensic Classifier Loss: 0.5277723670005798, WB Prediction: 0.5899176001548767\n",
            "[19/50] Loss: 0.7218927145004272 Perceptual Loss: 0.7136613130569458 L2 Loss: 10.755212783813477 Forensic Classifier Loss: 0.5707592964172363, WB Prediction: 0.5650961995124817\n",
            "[20/50] Loss: 0.7210789918899536 Perceptual Loss: 0.7122589349746704 L2 Loss: 11.41766357421875 Forensic Classifier Loss: 0.6222527623176575, WB Prediction: 0.5367339253425598\n",
            "[21/50] Loss: 0.7190731763839722 Perceptual Loss: 0.7102527618408203 L2 Loss: 12.152481079101562 Forensic Classifier Loss: 0.5488398671150208, WB Prediction: 0.5776195526123047\n",
            "[22/50] Loss: 0.7180261015892029 Perceptual Loss: 0.7085973024368286 L2 Loss: 12.901956558227539 Forensic Classifier Loss: 0.5955742597579956, WB Prediction: 0.551245927810669\n",
            "[23/50] Loss: 0.7170239090919495 Perceptual Loss: 0.70698082447052 L2 Loss: 13.679759979248047 Forensic Classifier Loss: 0.6406412720680237, WB Prediction: 0.5269544124603271\n",
            "[24/50] Loss: 0.7165241241455078 Perceptual Loss: 0.7059153318405151 L2 Loss: 14.444526672363281 Forensic Classifier Loss: 0.6773145794868469, WB Prediction: 0.5079792737960815\n",
            "[25/50] Loss: 0.715787947177887 Perceptual Loss: 0.7047467231750488 L2 Loss: 15.203540802001953 Forensic Classifier Loss: 0.6878820657730103, WB Prediction: 0.5026395320892334\n",
            "[26/50] Loss: 0.7156265377998352 Perceptual Loss: 0.7041682600975037 L2 Loss: 15.883567810058594 Forensic Classifier Loss: 0.703295886516571, WB Prediction: 0.4949513077735901\n",
            "[27/50] Loss: 0.7141989469528198 Perceptual Loss: 0.7030129432678223 L2 Loss: 16.556028366088867 Forensic Classifier Loss: 0.5815941691398621, WB Prediction: 0.5590065121650696\n",
            "[28/50] Loss: 0.7135663032531738 Perceptual Loss: 0.7018990516662598 L2 Loss: 17.281883239746094 Forensic Classifier Loss: 0.605259895324707, WB Prediction: 0.5459325313568115\n",
            "[29/50] Loss: 0.7124221920967102 Perceptual Loss: 0.700481653213501 L2 Loss: 17.928531646728516 Forensic Classifier Loss: 0.5952640175819397, WB Prediction: 0.5514169335365295\n",
            "[30/50] Loss: 0.7113217711448669 Perceptual Loss: 0.6989994645118713 L2 Loss: 18.489070892333984 Forensic Classifier Loss: 0.6155451536178589, WB Prediction: 0.5403462648391724\n",
            "[31/50] Loss: 0.710017204284668 Perceptual Loss: 0.6976345181465149 L2 Loss: 18.9536190032959 Forensic Classifier Loss: 0.5811761617660522, WB Prediction: 0.5592402219772339\n",
            "[32/50] Loss: 0.7091942429542542 Perceptual Loss: 0.6964071393013 L2 Loss: 19.496896743774414 Forensic Classifier Loss: 0.6077247262001038, WB Prediction: 0.544588565826416\n",
            "[33/50] Loss: 0.7074762582778931 Perceptual Loss: 0.6948063969612122 L2 Loss: 20.031795501708984 Forensic Classifier Loss: 0.5307936072349548, WB Prediction: 0.5881380438804626\n",
            "[34/50] Loss: 0.7060819268226624 Perceptual Loss: 0.6930757761001587 L2 Loss: 20.701152801513672 Forensic Classifier Loss: 0.5311136841773987, WB Prediction: 0.587949812412262\n",
            "[35/50] Loss: 0.7054266929626465 Perceptual Loss: 0.6919093132019043 L2 Loss: 21.321022033691406 Forensic Classifier Loss: 0.5713653564453125, WB Prediction: 0.5647538304328918\n",
            "[36/50] Loss: 0.7042520046234131 Perceptual Loss: 0.6902936697006226 L2 Loss: 21.90505599975586 Forensic Classifier Loss: 0.6011631488800049, WB Prediction: 0.5481736660003662\n",
            "[37/50] Loss: 0.7031659483909607 Perceptual Loss: 0.6890671849250793 L2 Loss: 22.493505477905273 Forensic Classifier Loss: 0.5704055428504944, WB Prediction: 0.5652961134910583\n",
            "[38/50] Loss: 0.702377200126648 Perceptual Loss: 0.6876255869865417 L2 Loss: 23.081554412841797 Forensic Classifier Loss: 0.6421696543693542, WB Prediction: 0.5261496305465698\n",
            "[39/50] Loss: 0.7011853456497192 Perceptual Loss: 0.6862804889678955 L2 Loss: 23.737565994262695 Forensic Classifier Loss: 0.6072103977203369, WB Prediction: 0.5448687076568604\n",
            "[40/50] Loss: 0.7003563046455383 Perceptual Loss: 0.6849378347396851 L2 Loss: 24.434789657592773 Forensic Classifier Loss: 0.6402170658111572, WB Prediction: 0.5271779894828796\n",
            "[41/50] Loss: 0.6992586851119995 Perceptual Loss: 0.6836948394775391 L2 Loss: 25.028820037841797 Forensic Classifier Loss: 0.6098875999450684, WB Prediction: 0.5434119701385498\n",
            "[42/50] Loss: 0.6988875865936279 Perceptual Loss: 0.6832069158554077 L2 Loss: 25.526613235473633 Forensic Classifier Loss: 0.5834639072418213, WB Prediction: 0.5579622983932495\n",
            "[43/50] Loss: 0.6982021331787109 Perceptual Loss: 0.682000458240509 L2 Loss: 25.947303771972656 Forensic Classifier Loss: 0.6456018686294556, WB Prediction: 0.5243468284606934\n",
            "[44/50] Loss: 0.6971752643585205 Perceptual Loss: 0.6809091567993164 L2 Loss: 26.340923309326172 Forensic Classifier Loss: 0.6191200017929077, WB Prediction: 0.5384180545806885\n",
            "[45/50] Loss: 0.6959182024002075 Perceptual Loss: 0.6797105669975281 L2 Loss: 26.753456115722656 Forensic Classifier Loss: 0.566185474395752, WB Prediction: 0.5676867365837097\n",
            "[46/50] Loss: 0.6955881714820862 Perceptual Loss: 0.6789841055870056 L2 Loss: 27.158327102661133 Forensic Classifier Loss: 0.6049723625183105, WB Prediction: 0.5460895299911499\n",
            "[47/50] Loss: 0.6951377987861633 Perceptual Loss: 0.6783474683761597 L2 Loss: 27.51262664794922 Forensic Classifier Loss: 0.6068021655082703, WB Prediction: 0.5450912117958069\n",
            "[48/50] Loss: 0.6949551105499268 Perceptual Loss: 0.6780060529708862 L2 Loss: 27.917757034301758 Forensic Classifier Loss: 0.5980358719825745, WB Prediction: 0.5498906373977661\n",
            "[49/50] Loss: 0.6943352222442627 Perceptual Loss: 0.6773655414581299 L2 Loss: 28.236452102661133 Forensic Classifier Loss: 0.5702903866767883, WB Prediction: 0.5653612613677979\n",
            "[50/50] Loss: 0.6937095522880554 Perceptual Loss: 0.6765867471694946 L2 Loss: 28.60986328125 Forensic Classifier Loss: 0.5635725259780884, WB Prediction: 0.5691720247268677\n"
          ]
        }
      ],
      "source": [
        "! python image_as_reference.py --config_file configs/config_image_as_reference.ini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWu5gphGBJnb",
        "outputId": "de7930bf-c40a-4cc1-c9b0-85f0a32beab8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\r  0% 0/6 [00:00<?, ?it/s]\rProcessing the ('densenet121', 'resnet50', 'resnet18', 'vgg19', 'efficientnet') combination:   0% 0/6 [00:00<?, ?it/s]\rProcessing the ('densenet121', 'resnet50', 'resnet18', 'vgg19', 'efficientnet') combination:   0% 0/6 [00:00<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/face_attribute_attack/adversarial_transferability.py\", line 653, in <module>\n",
            "    accuracy_metrics = meta_classifier(os.path.join(config.get('DATA_PATHS', 'base_latent_path'), 'initial_latents_and_prompts.pt'), wb_model_combination, all_wb_model_names, config.get('DATA_PATHS', 'train_log_path'))\n",
            "  File \"/content/face_attribute_attack/adversarial_transferability.py\", line 445, in meta_classifier\n",
            "    stored_latents, stored_text_prompts, stored_chosen_prompt_strings = load_start_latents(start_latents_path)\n",
            "  File \"/content/face_attribute_attack/adversarial_transferability.py\", line 223, in load_start_latents\n",
            "    data = torch.load(start_latents_path)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 998, in load\n",
            "    with _open_file_like(f, 'rb') as opened_file:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 445, in _open_file_like\n",
            "    return _open_file(name_or_buffer, mode)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 426, in __init__\n",
            "    super().__init__(open(name, mode))\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'results/base_latents/initial_latents_and_prompts.pt'\n"
          ]
        }
      ],
      "source": [
        "! python adversarial_transferability.py --config_file configs/config_adversarial_transferrability.ini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Gxqx6fUeBLU6"
      },
      "outputs": [],
      "source": [
        "! cp /content/face_attribute_attack -r /content/drive/MyDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGXXfjSpLUBS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
